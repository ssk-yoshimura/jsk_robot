{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6442df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b57872",
   "metadata": {},
   "source": [
    "レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149f33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "  # W,b,f,u,x_output,delta,eta # node  input: n output: m,  batch l\n",
    "  def __init__(self, W, b, f, df_y, eta):\n",
    "    self.W = W # (m,n)\n",
    "    self.b = b # (m)\n",
    "    self.f = f \n",
    "    self.df_y = df_y\n",
    "    self.eta = eta\n",
    "    self.m = self.W.shape[0]\n",
    "    self.n = self.W.shape[1]\n",
    "\n",
    "  def forward_old(self, x_input): # x_input: (n,l), x_output,u: (m,l)\n",
    "    self.x_input = x_input\n",
    "    self.l = self.x_input.shape[1]\n",
    "    self.u = (np.dot(self.W, x_input).T + self.b).T\n",
    "    self.x_output = self.f(self.u)\n",
    "    return self.x_output\n",
    "\n",
    "  def forward(self, x_input, dropout_p = 0.0): # x_input: (n,l), x_output,u: (m,l)\n",
    "    self.x_input = x_input\n",
    "    self.l = self.x_input.shape[1]\n",
    "    self.u = (np.dot(self.W, x_input).T + self.b).T\n",
    "    self.x_output = self.f(self.u)\n",
    "    if dropout_p != 0.0:\n",
    "        dropout_mask = np.random.random_sample(self.x_output.shape)\n",
    "        dropout_mask[dropout_mask < dropout_p] = 0.0\n",
    "        dropout_mask[dropout_mask > 0.0] = 1.0\n",
    "        self.x_output = self.x_output * dropout_mask\n",
    "    return self.x_output\n",
    "\n",
    "  def bp(self, c): # c: (m,l)\n",
    "    self.delta = c * self.df_y(self.x_output); # (m,l)\n",
    "    return np.dot(self.W.T, self.delta) # (n,l)\n",
    "\n",
    "  def update(self):\n",
    "    self.W = self.W - self.eta * np.dot(self.delta, self.x_input.T) / self.l\n",
    "    self.b = self.b - self.eta * np.mean(self.delta, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53faa97",
   "metadata": {},
   "source": [
    "NNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfa66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn:\n",
    "    def __init__(self, node_list, out_func=lambda x: x):\n",
    "        self.out_func = out_func\n",
    "        self.layerlist = []\n",
    "        self.node_list = node_list\n",
    "        self.state_n = node_list[0]\n",
    "        self.action_n = node_list[-1]\n",
    "        self.dropout_list = np.zeros(len(node_list)-1)\n",
    "        for i in range(len(node_list)-1):\n",
    "            self.layerlist.append(layer(np.random.randn(node_list[i+1], node_list[i])*0.03,\n",
    "                                        np.random.randn(node_list[i+1])*0.03, \n",
    "                                        lambda x : 1/(1 + np.exp(-x)),\n",
    "                                        lambda y : y*(1-y),\n",
    "                                        0.5))\n",
    "        self.set_W_n()\n",
    "\n",
    "    def set_dropout(self, drop_list):\n",
    "        self.dropout_list = drop_list\n",
    "  \n",
    "    def set_W(self, dev_list):\n",
    "        for i in range(len(dev_list)):\n",
    "            self.layerlist[i].W = np.random.standard_normal(self.layerlist[i].W.shape)*dev_list[i]\n",
    "            self.layerlist[i].b = np.random.standard_normal(self.layerlist[i].b.shape)*dev_list[i]\n",
    "  \n",
    "    def set_W_n(self):\n",
    "        dev_list = []\n",
    "        for i in range(len(self.layerlist)):\n",
    "            dev_list.append(1/np.sqrt(self.layerlist[i].W.shape[1]))\n",
    "            self.set_W(dev_list)\n",
    "            # print(dev_list)\n",
    "\n",
    "    def set_func(self, func_list):\n",
    "        for i in range(len(func_list)):\n",
    "            self.layerlist[i].f = func_list[i][0]\n",
    "            self.layerlist[i].df_y = func_list[i][1]\n",
    "  \n",
    "    def set_func_relu(self):\n",
    "        for i in range(len(self.layerlist)-1):\n",
    "            self.layerlist[i].f = relu()[0]\n",
    "            self.layerlist[i].df_y = relu()[1]\n",
    "        self.layerlist[-1].f = identify()[0]\n",
    "        self.layerlist[-1].df_y = identify()[1]\n",
    "\n",
    "    def set_func_sigmoid(self):\n",
    "        for i in range(len(self.layerlist)):\n",
    "            self.layerlist[i].f = sigmoid()[0]\n",
    "            self.layerlist[i].df_y = sigmoid()[1]\n",
    "\n",
    "    def set_l_rate(self, l_list):\n",
    "        for i in range(len(l_list)):\n",
    "            self.layerlist[i].eta = l_list[i] \n",
    "    \n",
    "    def set_rate_all(self, l_rate):\n",
    "        for i in range(len(self.layerlist)):\n",
    "            self.layerlist[i].eta = l_rate\n",
    "  \n",
    "    def forward(self, state): # state:(n,1), n:state size = NN input size\n",
    "        x = state\n",
    "        x_record = [] # record x for learning\n",
    "        for l in self.layerlist:\n",
    "            x_record.append(x)\n",
    "            x = l.forward(x)\n",
    "        x = self.out_func(x) # use out_func\n",
    "        x_record.append(x)\n",
    "        action = np.argmax(x, axis=0)\n",
    "        return x_record, action\n",
    "    \n",
    "    def forward_old(self, state):\n",
    "        x = state\n",
    "        for l in self.layerlist:\n",
    "            x = l.forward(x)\n",
    "        x = self.out_func(x)\n",
    "        maxQ = np.max(x, axis=0)\n",
    "        return maxQ\n",
    "\n",
    "    def collect_init(self):\n",
    "        self.x_collect = [[] for _ in range(len(self.layerlist)+1)]\n",
    "\n",
    "    def play(self, state, epsilon):\n",
    "        self.x_record, self.action_best = self.forward(state)\n",
    "        if epsilon > np.random.uniform(0,1):\n",
    "            next_action = np.random.choice(self.action_n, self.action_best.size)\n",
    "        else:\n",
    "            next_action = self.action_best\n",
    "        return next_action\n",
    "    \n",
    "    def collect(self):\n",
    "        for i in range(len(self.x_collect)):\n",
    "            self.x_collect[i].append(self.x_record[i][:,0])\n",
    "        # print(self.x_collect)\n",
    "        return self.x_collect\n",
    "    \n",
    "    def collect_end(self):\n",
    "        self.x_collect_arr = []\n",
    "        for i in range(len(self.x_collect)):\n",
    "            self.x_collect_arr.append(np.array(self.x_collect[i]).T)\n",
    "        # print(self.x_collect_arr)\n",
    "        return self.x_collect_arr\n",
    "    \n",
    "    def update(self, label):\n",
    "        # layerのxをx_recで書き換える\n",
    "        for i in range(len(self.layerlist)):\n",
    "            self.layerlist[i].x_input = self.x_collect_arr[i]\n",
    "            self.layerlist[i].x_output = self.x_collect_arr[i+1]\n",
    "        # bp\n",
    "        c = self.x_collect_arr[-1] - label\n",
    "        for l in reversed(self.layerlist):\n",
    "            c = l.bp(c)\n",
    "        for l in self.layerlist:\n",
    "            l.update()\n",
    "\n",
    "def relu(k=1.0):\n",
    "    def relu_func(x):\n",
    "        mask = (x<0)\n",
    "        x[mask] = 0\n",
    "        return k*x\n",
    "    return (relu_func, lambda y:k*(y>0))\n",
    "\n",
    "def identify():\n",
    "    return (lambda x:x, lambda y:1)\n",
    "\n",
    "def sigmoid(alpha=1.0):\n",
    "    return (lambda x:1/(1+np.exp(-alpha*x)), lambda y:alpha*y*(1-y))\n",
    "\n",
    "def tanh():\n",
    "    return (lambda x:np.tanh(x), lambda y:(1-y*y))\n",
    "\n",
    "def softmax(x):\n",
    "    x_max = np.max(x, axis=0)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    sum_e = np.sum(exp_x, axis=0)\n",
    "    return exp_x / sum_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "65ef76a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnvSim():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.state_table = np.array([[1,0],[0,1]])\n",
    "        self.reward_table = np.array([[0,0],[0,1]])\n",
    "\n",
    "    def reset(self):\n",
    "        # self._state = np.array([[0],[0]]).T # do not use this\n",
    "        self._state = np.array([[0]])\n",
    "        return self._state\n",
    "\n",
    "    def step(self, action):\n",
    "        self._state = np.array(self.state_table[self._state, action])\n",
    "        reward = self.reward_table[self._state, action]\n",
    "        done = False\n",
    "        return self._state, reward, done\n",
    "    \n",
    "    def get_rewards(self, states): # states:(n,T), return (m,T) n:state,m:action\n",
    "        return self.reward_table[states][0].T\n",
    "        \n",
    "    def get_nextS(self, states): # states:(n,T), return nextS:(m,T)\n",
    "        return self.state_table[states][0].T\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5809e31",
   "metadata": {},
   "source": [
    "1ループについて試す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a1cf892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 1]]\n",
      "[[0 0]\n",
      " [0 0]]\n",
      "[[0. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "net = dqn([1,4,4,2])\n",
    "#net.set_func([identify(),identify(),identify()])\n",
    "net.set_func([tanh(), tanh(), identify()])\n",
    "env = MyEnvSim()\n",
    "#state = net.state_reset(env.reset()) # いらない\n",
    "#loop\n",
    "env.reset()\n",
    "net.collect_init()\n",
    "\n",
    "#loop\n",
    "action = net.play(state, 0.3)\n",
    "#print(action)\n",
    "next_state, reward, done = env.step(action)\n",
    "net.collect()\n",
    "state = next_state\n",
    "action = net.play(state, 0.3)\n",
    "#print(action)\n",
    "next_state, reward, done = env.step(action)\n",
    "a = net.collect()\n",
    "state = next_state\n",
    "#np.array(a)\n",
    "#np.array(a[1])\n",
    "\n",
    "#loop end\n",
    "x_all = net.collect_end()\n",
    "# get reward\n",
    "r_all = env.get_rewards(x_all[0])\n",
    "print(r_all)\n",
    "sn_all = env.get_nextS(x_all[0])\n",
    "#print(sn_all)\n",
    "data_oldQ_input = sn_all.ravel(order='F').reshape(1,sn_all.size)\n",
    "#print(data_oldQ_input)\n",
    "maxQold = net.forward_old(data_oldQ_input).reshape(sn_all.shape, order='F')\n",
    "print(maxQold)\n",
    "\n",
    "gamma = 0.2\n",
    "d = r_all + gamma * maxQold\n",
    "print(d) # ラベル\n",
    "\n",
    "# bp\n",
    "net.update(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc78c69",
   "metadata": {},
   "source": [
    "ループでやってみる\n",
    "\n",
    "成功例\n",
    "\n",
    "(1,10,10,2), identify, lrate0.3, gamma0.5, epsilon=0.5 * (1 / (i+1))\n",
    "\n",
    "上と同じ、(tanh,tanh,identify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1a696038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]] [0, 0, 0, 0, 0, 0, 0]\n",
      "[[3]] [1, 0, 0, 0, 1, 1, 1]\n",
      "[[0]] [1, 1, 1, 1, 1, 1, 1]\n",
      "[[0]] [0, 0, 0, 0, 0, 0, 0]\n",
      "[[0]] [1, 1, 1, 1, 1, 1, 1]\n",
      "[[5]] [1, 0, 1, 1, 1, 1, 1]\n",
      "[[0]] [1, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[0]] [1, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[0]] [1, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[0]] [1, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[0]] [1, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[5]] [0, 1, 1, 1, 1, 1, 0]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[4]] [0, 1, 1, 1, 0, 0, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n",
      "[[6]] [0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "net = dqn([1,10,10,2])\n",
    "# net.set_func([identify(),identify(),identify()])\n",
    "net.set_func([tanh(), tanh(), identify()])\n",
    "net.set_rate_all(0.3)\n",
    "env = MyEnvSim()\n",
    "gamma = 0.5\n",
    "\n",
    "for i in range(50):\n",
    "    state = env.reset()\n",
    "    net.collect_init()\n",
    "    epsilon = 0.5 * (1 / (i+1))\n",
    "    reward_sum = 0\n",
    "    action_list = []\n",
    "    for j in range(7):\n",
    "        action = net.play(state, epsilon) # calc NN and select action using e-greedy\n",
    "        action_list.append(action[0])\n",
    "        next_state, reward, done = env.step(action)\n",
    "        net.collect() # record x\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "    x_all = net.collect_end() # all x\n",
    "    r_all = env.get_rewards(x_all[0]) # all r\n",
    "    sn_all = env.get_nextS(x_all[0]) # all nextS\n",
    "    data_oldQ_input = sn_all.ravel(order='F').reshape(1,sn_all.size)\n",
    "    maxQold = net.forward_old(data_oldQ_input).reshape(sn_all.shape, order='F')\n",
    "    #print(x_all)\n",
    "    label = r_all + gamma * maxQold\n",
    "    #print(label)\n",
    "    net.update(label)\n",
    "    print(reward_sum, action_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a3a12",
   "metadata": {},
   "source": [
    "### プログラムの説明\n",
    "\n",
    "外側のループ\n",
    "\n",
    "1. 環境のリセット\n",
    "1. NNのニューロンの値保存の初期化\n",
    "1. epsilonの設定\n",
    "1. ゲームをプレイする（ループ）\n",
    "1. プレイ中のすべてのニューロンの値Xを得る\n",
    "1. プレイ中の各状態sについて、全てのaを選択したときの報酬、次のs'をそれぞれ計算する\n",
    "1. 上で求めた次のs'から、maxQ(s')を求める\n",
    "1. 上で求めた報酬とmaxQ(s')でラベルLを作成\n",
    "1. XとLを使って誤差逆伝播\n",
    "\n",
    "内側のループ（ゲームプレイ）\n",
    "\n",
    "1. 状態とNNから、epsilon-greedyで行動を選択する\n",
    "1. 環境にactionを入力し、次の状態と報酬を得る\n",
    "1. NNのニューロンの値xを記録する\n",
    "\n",
    "#### envに必要な機能\n",
    "\n",
    "1. reset: 状態をリセットする\n",
    "1. step: 行動を一つ指定したとき、状態を遷移させ、（次の状態、報酬、終了判定）を返す\n",
    "1. get_rewards: 状態を指定したとき、全ての行動に対する報酬を返す\n",
    "1. get_nextS: 状態を指定したとき、全ての行動に対する次の状態を返す\n",
    "\n",
    "get_rewardsは、状態がn個の変数で表されるとき、T個の状態を表した(n,T)の行列に対して、(m,T)の行列を返す必要がある(mは行動の変数の個数）。\n",
    "\n",
    "get_nextSも同様。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bf017",
   "metadata": {},
   "source": [
    "### 迷路をDQNで解く\n",
    "\n",
    "通路上での報酬も負にすると、うまくいきやすい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef6ba7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMaze():\n",
    "    def __init__(self, maze, wall_reward, start, goal):\n",
    "        self.maze = maze\n",
    "        self.wall_reward = wall_reward\n",
    "        self.maze_ravel = self.maze.ravel()\n",
    "        self.h = self.maze.shape[0]\n",
    "        self.w = self.maze.shape[1]\n",
    "        self.start = start[0]*self.w + start[1]\n",
    "        self.goal = goal[0]*self.w + goal[1]\n",
    "        # print(self.start)\n",
    "        # print(self.goal)\n",
    "        self.move = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "        self.maze_size = maze.size\n",
    "        # print(self.maze_size)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.maze.size)\n",
    "        self.state[self.start] = 1\n",
    "        return self.state\n",
    "     \n",
    "    def step_all(self, state): # state:(h*w), nxt_num:(m), nxt_rw:(m)\n",
    "        num = np.where(state==1)[0][0] # 状態の番号0~8\n",
    "        idx = np.array([num//self.w, num%self.w]) #状態の場所 (2)\n",
    "        nxt = self.move + idx # 移動先の場所 (4,2)\n",
    "        is_wall_h = (nxt[:,0] >= self.h) | (nxt[:,0] < 0)\n",
    "        is_wall_w = (nxt[:,1] >= self.w) | (nxt[:,1] < 0)\n",
    "        is_wall = is_wall_h | is_wall_w\n",
    "        nxt[is_wall] = idx\n",
    "        nxt_num = nxt[:,0] * self.w + nxt[:,1] # 移動先の番号 (4)\n",
    "        nxt_rw = self.maze_ravel[nxt_num] # 移動先の報酬 (4)\n",
    "        nxt_num[is_wall] = num # 壁なら番号をもとのまま\n",
    "        nxt_rw[is_wall] = self.wall_reward # 壁なら報酬0\n",
    "        return nxt_num, nxt_rw\n",
    "    \n",
    "    def step_all_multi(self, states): # states,(h*w,T), return nextS_num:(mT), rewards:(mT)\n",
    "        n = states.shape[0]\n",
    "        T = states.shape[1]\n",
    "        msk = states.T == 1\n",
    "        Z = np.zeros((T,n))\n",
    "        B = np.arange(n)\n",
    "        Z += B\n",
    "        snum = Z[msk]\n",
    "        idx = np.array([snum//self.w,snum%self.w]).T \n",
    "        nxt = np.hstack([idx+move[0], idx+move[1], idx+move[2], idx+move[3]]).reshape(4*T, 2)\n",
    "        bf =  np.hstack([idx,idx,idx,idx]).reshape(4*T, 2)\n",
    "        is_wall_h = (nxt[:,0] >= self.h) | (nxt[:,0] < 0)\n",
    "        is_wall_w = (nxt[:,1] >= self.w) | (nxt[:,1] < 0)\n",
    "        is_wall = is_wall_h | is_wall_w\n",
    "        nxt[is_wall] = bf[is_wall]\n",
    "        nxt_num = nxt[:,0] * self.w + nxt[:,1] # 移動後の番号 (mT)\n",
    "        # print(nxt_num)\n",
    "        nxt_rw = self.maze_ravel[nxt_num.astype('int32')] # 移動先の報酬 (mT)\n",
    "        nxt_rw[is_wall] = self.wall_reward # 壁なら報酬0\n",
    "        return nxt_num, nxt_rw.reshape((4,T), order='F')\n",
    "    \n",
    "    def step(self, action): # action:0~3\n",
    "        nxt_num, nxt_rw = self.step_all(self.state)\n",
    "        nxtn = nxt_num[action]\n",
    "        nxtr = nxt_rw[action]\n",
    "        self.state = np.zeros(self.maze.size)\n",
    "        self.state[nxtn] = 1\n",
    "        done = False\n",
    "        if nxtn == self.goal:\n",
    "            done = True\n",
    "        return self.state, nxtr, done\n",
    "    \n",
    "    def get_R_and_NS(self, states): # states:(n,T)\n",
    "        nextS_list = []\n",
    "        reward_list = []\n",
    "        for i in range(states.shape[1]):\n",
    "            nxtn, nxtr = self.step_all(states[:,i])\n",
    "            nextS_list.append(nxtn)\n",
    "            reward_list.append(nxtr)\n",
    "        nextS = np.array(nextS_list).T\n",
    "        reward = np.array(reward_list).T\n",
    "        return nextS, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdb63ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.0\n",
      "0.5\n",
      "-137.0\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "maze = np.array([[-0.5,-1,-1],\n",
    "                 [-0.5,-1,-1],\n",
    "                 [-0.5,-0.5,2]])\n",
    "wall_reward = -2\n",
    "start = [0,0]\n",
    "goal = [2,2]\n",
    "env = MyMaze(maze, wall_reward, start, goal)\n",
    "maze_size = env.maze_size\n",
    "net = dqn([maze_size,20,20,4])\n",
    "net.set_func([tanh(), tanh(), identify()])\n",
    "net.set_rate_all(0.3)\n",
    "gamma = 0.5\n",
    "\n",
    "for i in range(101):\n",
    "    state = env.reset()\n",
    "    net.collect_init()\n",
    "    epsilon = 0.5 * (1 / (i+1))\n",
    "    reward_sum = 0\n",
    "    action_list = []\n",
    "    for j in range(70):\n",
    "        action = net.play(state.reshape(state.size, 1), epsilon) # calc NN and select action using e-greedy\n",
    "        # action_list.append(action[0])\n",
    "        next_state, reward, done = env.step(action[0])\n",
    "        # print(next_state, reward, done)\n",
    "        net.collect() # record x\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print(reward_sum)\n",
    "    x_all = net.collect_end() # all x\n",
    "    nextS, reward = env.step_all_multi(x_all[0])\n",
    "    nextS_n_mT = np.zeros((maze_size, nextS.size))\n",
    "    nextS_n_mT[nextS.astype('int32'), np.arange(nextS.size)] = 1\n",
    "    maxQold = net.forward_old(nextS_n_mT).reshape(reward.shape, order='F')\n",
    "    label = reward + gamma * maxQold\n",
    "    net.update(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b88381c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-105.0\n",
      "-137.0\n",
      "-138.0\n",
      "-1.0\n",
      "-52.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "maze = np.array([[-0.5,-1,-1],\n",
    "                 [-0.5,-1,-1],\n",
    "                 [-0.5,-0.5,2]])\n",
    "wall_reward = -2\n",
    "start = [0,0]\n",
    "goal = [2,2]\n",
    "env = MyMaze(maze, wall_reward, start, goal)\n",
    "maze_size = env.maze_size\n",
    "net = dqn([maze_size,20,20,4])\n",
    "net.set_func([tanh(), tanh(), identify()])\n",
    "net.set_rate_all(0.3)\n",
    "gamma = 0.5\n",
    "\n",
    "for i in range(100):\n",
    "    state = env.reset()\n",
    "    net.collect_init()\n",
    "    epsilon = 0.5 * (1 / (i+1))\n",
    "    reward_sum = 0\n",
    "    action_list = []\n",
    "    for j in range(70):\n",
    "        action = net.play(state.reshape(state.size, 1), epsilon) # calc NN and select action using e-greedy\n",
    "        # action_list.append(action[0])\n",
    "        next_state, reward, done = env.step(action[0])\n",
    "        # print(next_state, reward, done)\n",
    "        net.collect() # record x\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print(reward_sum)\n",
    "    x_all = net.collect_end() # all x\n",
    "    nextS, reward = env.get_R_and_NS(x_all[0])\n",
    "    nextS_ravel = nextS.ravel(order='F')\n",
    "    nextS_n_mT = np.zeros((maze_size, nextS_ravel.size))\n",
    "    nextS_n_mT[nextS_ravel, np.arange(nextS_ravel.size)] = 1\n",
    "    maxQold = net.forward_old(nextS_n_mT).reshape(reward.shape, order='F')\n",
    "    label = reward + gamma * maxQold\n",
    "    net.update(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a83377ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.5\n",
      "-50.0\n",
      "-50.0\n",
      "7.5\n",
      "7.5\n"
     ]
    }
   ],
   "source": [
    "maze = np.array([[-0.5,-0.5,-0.5,-1],\n",
    "                 [-1,-0.5,-1,-1],\n",
    "                 [-1,-0.5,-1,-1],\n",
    "                 [-0.5,-0.5,-0.5,10]])\n",
    "wall_reward = -2\n",
    "start = [0,0]\n",
    "goal = [3,3]\n",
    "env = MyMaze(maze, wall_reward, start, goal)\n",
    "maze_size = env.maze_size\n",
    "net = dqn([maze_size,40,40,4])\n",
    "net.set_func([tanh(), tanh(), identify()])\n",
    "net.set_rate_all(0.2)\n",
    "gamma = 0.5\n",
    "\n",
    "for i in range(401):\n",
    "    state = env.reset()\n",
    "    net.collect_init()\n",
    "    epsilon = 0.5 * (1 / (i+1))\n",
    "    reward_sum = 0\n",
    "    action_list = []\n",
    "    for j in range(100):\n",
    "        action = net.play(state.reshape(state.size, 1), epsilon) # calc NN and select action using e-greedy\n",
    "        # action_list.append(action[0])\n",
    "        next_state, reward, done = env.step(action[0])\n",
    "        # print(next_state, reward, done)\n",
    "        net.collect() # record x\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            # print(done)\n",
    "            break\n",
    "    if i%100 == 0:\n",
    "        print(reward_sum)\n",
    "    x_all = net.collect_end() # all x\n",
    "    nextS, reward = env.get_R_and_NS(x_all[0])\n",
    "    nextS_ravel = nextS.ravel(order='F')\n",
    "    nextS_n_mT = np.zeros((maze_size, nextS_ravel.size))\n",
    "    nextS_n_mT[nextS_ravel, np.arange(nextS_ravel.size)] = 1\n",
    "    maxQold = net.forward_old(nextS_n_mT).reshape(reward.shape, order='F')\n",
    "    label = reward + gamma * maxQold\n",
    "    net.update(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "563371b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-65.5\n",
      "6.0\n",
      "7.0\n",
      "7.5\n",
      "7.5\n"
     ]
    }
   ],
   "source": [
    "maze = np.array([[-0.5,-0.5,-0.5,-1],\n",
    "                 [-1,-0.5,-1,-1],\n",
    "                 [-1,-0.5,-1,-1],\n",
    "                 [-0.5,-0.5,-0.5,10]])\n",
    "wall_reward = -2\n",
    "start = [0,0]\n",
    "goal = [3,3]\n",
    "env = MyMaze(maze, wall_reward, start, goal)\n",
    "maze_size = env.maze_size\n",
    "net = dqn([maze_size,40,40,4])\n",
    "net.set_func([tanh(), tanh(), identify()])\n",
    "net.set_rate_all(0.2)\n",
    "gamma = 0.3\n",
    "\n",
    "for i in range(401):\n",
    "    state = env.reset()\n",
    "    net.collect_init()\n",
    "    epsilon = 0.5 * (1 / (i+1))\n",
    "    reward_sum = 0\n",
    "    action_list = []\n",
    "    for j in range(50):\n",
    "        action = net.play(state.reshape(state.size, 1), epsilon) # calc NN and select action using e-greedy\n",
    "        # action_list.append(action[0])\n",
    "        next_state, reward, done = env.step(action[0])\n",
    "        # print(next_state, reward, done)\n",
    "        net.collect() # record x\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            # print(done)\n",
    "            break\n",
    "    if i%100 == 0:\n",
    "        print(reward_sum)\n",
    "    x_all = net.collect_end() # all x\n",
    "    nextS, reward = env.step_all_multi(x_all[0])\n",
    "    nextS_n_mT = np.zeros((maze_size, nextS.size))\n",
    "    nextS_n_mT[nextS.astype('int32'), np.arange(nextS.size)] = 1\n",
    "    maxQold = net.forward_old(nextS_n_mT).reshape(reward.shape, order='F')\n",
    "    label = reward + gamma * maxQold\n",
    "    net.update(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
